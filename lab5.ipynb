{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №5 (Проведение исследований с градиентным бустингом)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Создание бейзлайна и оценка качества\n",
    "**2.a** Обучить модели из `sklearn` (для классификации и регрессии) для выбранных наборов данных  \n",
    "**2.b** Оценить качество моделей (для классификации и регрессии) по выбранным метрикам\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             mean_squared_error, mean_absolute_error, r2_score)\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (395, 33)\n",
      "Бейзлайн GradientBoosting (Classification):\n",
      "Accuracy:  0.8734\n",
      "Precision: 0.9388\n",
      "Recall:    0.8679\n",
      "F1-score:  0.9020\n",
      "\n",
      "Бейзлайн GradientBoosting (Regression):\n",
      "MSE:  2.9156\n",
      "MAE:  1.0626\n",
      "R^2:  0.8578\n"
     ]
    }
   ],
   "source": [
    "# 1. Загрузка данных\n",
    "data = pd.read_csv('student_data.csv', sep=',')\n",
    "print(\"Shape:\", data.shape)\n",
    "\n",
    "# (Опционально) Удаляем пропуски, если есть\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# 2. Бинарная классификация: passed = 1 (G3 >= 10) / 0 иначе\n",
    "data_class = data.copy()\n",
    "data_class['passed'] = (data_class['G3'] >= 10).astype(int)\n",
    "\n",
    "X_class = data_class[['G1', 'G2', 'studytime', 'failures', 'absences']]\n",
    "y_class = data_class['passed']\n",
    "\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
    ")\n",
    "\n",
    "# Бейзлайн (Classification)\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "gb_clf.fit(Xc_train, yc_train)\n",
    "yc_pred = gb_clf.predict(Xc_test)\n",
    "\n",
    "acc = accuracy_score(yc_test, yc_pred)\n",
    "prec = precision_score(yc_test, yc_pred)\n",
    "rec = recall_score(yc_test, yc_pred)\n",
    "f1 = f1_score(yc_test, yc_pred)\n",
    "\n",
    "print(\"Бейзлайн GradientBoosting (Classification):\")\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")\n",
    "\n",
    "# 3. Регрессия: предсказываем G3\n",
    "X_reg = data[['G1', 'G2', 'studytime', 'failures', 'absences']]\n",
    "y_reg = data['G3']\n",
    "\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Бейзлайн (Regression)\n",
    "gb_reg = GradientBoostingRegressor(random_state=42)\n",
    "gb_reg.fit(Xr_train, yr_train)\n",
    "yr_pred = gb_reg.predict(Xr_test)\n",
    "\n",
    "mse = mean_squared_error(yr_test, yr_pred)\n",
    "mae = mean_absolute_error(yr_test, yr_pred)\n",
    "r2 = r2_score(yr_test, yr_pred)\n",
    "\n",
    "print(\"\\nБейзлайн GradientBoosting (Regression):\")\n",
    "print(f\"MSE:  {mse:.4f}\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"R^2:  {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**:  \n",
    "- Создан бейзлайн градиентного бустинга для классификации (метрики: accuracy, precision, recall, f1).  \n",
    "- Создан бейзлайн градиентного бустинга для регрессии (метрики: MSE, MAE, R^2).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Улучшение бейзлайна\n",
    "**3.a** Сформулировать гипотезы (препроцессинг, новые признаки, подбор гиперпараметров…)  \n",
    "**3.b** Проверить гипотезы  \n",
    "**3.c** Сформировать улучшенный бейзлайн  \n",
    "**3.d** Обучить модели (классификация и регрессия)  \n",
    "**3.e** Оценить качество  \n",
    "**3.f** Сравнить с пунктом 2  \n",
    "**3.g** Сделать выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Улучшенный бейзлайн (GradientBoosting - Classification):\n",
      "Лучшие параметры: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Accuracy:  0.9241\n",
      "Precision: 0.9796\n",
      "Recall:    0.9057\n",
      "F1-score:  0.9412\n",
      "\n",
      "Улучшенный бейзлайн (GradientBoosting - Regression):\n",
      "Лучшие параметры: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.8}\n",
      "MSE:  1.7844\n",
      "MAE:  0.9020\n",
      "R^2:  0.8724\n"
     ]
    }
   ],
   "source": [
    "# 1) Гипотеза: Удалим выбросы (absences>40) и добавим признак total_G1_G2\n",
    "data_improved = data.copy()\n",
    "data_improved = data_improved[data_improved['absences'] <= 40]\n",
    "\n",
    "data_improved['total_G1_G2'] = data_improved['G1'] + data_improved['G2']\n",
    "\n",
    "# ----- КЛАССИФИКАЦИЯ -----\n",
    "data_class2 = data_improved.copy()\n",
    "data_class2['passed'] = (data_class2['G3'] >= 10).astype(int)\n",
    "X_class2 = data_class2[['G1','G2','studytime','failures','absences','total_G1_G2']]\n",
    "y_class2 = data_class2['passed']\n",
    "\n",
    "Xc2_train, Xc2_test, yc2_train, yc2_test = train_test_split(\n",
    "    X_class2, y_class2, test_size=0.2, random_state=42, stratify=y_class2\n",
    ")\n",
    "\n",
    "# Подбор гиперпараметров для классификации\n",
    "param_grid_clf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [1.0, 0.8],\n",
    "}\n",
    "gb_clf_cv = GridSearchCV(GradientBoostingClassifier(random_state=42),\n",
    "                         param_grid_clf, cv=5, scoring='f1', n_jobs=-1)\n",
    "gb_clf_cv.fit(Xc2_train, yc2_train)\n",
    "\n",
    "best_clf = gb_clf_cv.best_estimator_\n",
    "yc2_pred = best_clf.predict(Xc2_test)\n",
    "\n",
    "acc_best = accuracy_score(yc2_test, yc2_pred)\n",
    "prec_best = precision_score(yc2_test, yc2_pred)\n",
    "rec_best = recall_score(yc2_test, yc2_pred)\n",
    "f1_best = f1_score(yc2_test, yc2_pred)\n",
    "\n",
    "print(\"Улучшенный бейзлайн (GradientBoosting - Classification):\")\n",
    "print(\"Лучшие параметры:\", gb_clf_cv.best_params_)\n",
    "print(f\"Accuracy:  {acc_best:.4f}\")\n",
    "print(f\"Precision: {prec_best:.4f}\")\n",
    "print(f\"Recall:    {rec_best:.4f}\")\n",
    "print(f\"F1-score:  {f1_best:.4f}\")\n",
    "\n",
    "# ----- РЕГРЕССИЯ -----\n",
    "X_reg2 = data_improved[['G1','G2','studytime','failures','absences','total_G1_G2']]\n",
    "y_reg2 = data_improved['G3']\n",
    "\n",
    "Xr2_train, Xr2_test, yr2_train, yr2_test = train_test_split(\n",
    "    X_reg2, y_reg2, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "param_grid_reg = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [1.0, 0.8],\n",
    "}\n",
    "gb_reg_cv = GridSearchCV(GradientBoostingRegressor(random_state=42),\n",
    "                         param_grid_reg, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "gb_reg_cv.fit(Xr2_train, yr2_train)\n",
    "\n",
    "best_reg = gb_reg_cv.best_estimator_\n",
    "yr2_pred = best_reg.predict(Xr2_test)\n",
    "\n",
    "mse_best = mean_squared_error(yr2_test, yr2_pred)\n",
    "mae_best = mean_absolute_error(yr2_test, yr2_pred)\n",
    "r2_best = r2_score(yr2_test, yr2_pred)\n",
    "\n",
    "print(\"\\nУлучшенный бейзлайн (GradientBoosting - Regression):\")\n",
    "print(\"Лучшие параметры:\", gb_reg_cv.best_params_)\n",
    "print(f\"MSE:  {mse_best:.4f}\")\n",
    "print(f\"MAE:  {mae_best:.4f}\")\n",
    "print(f\"R^2:  {r2_best:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Имплементация алгоритма машинного обучения (Градиентный бустинг)\n",
    "\n",
    "**4.a** Самостоятельно имплементировать алгоритмы (для классификации и регрессии)  \n",
    "**4.b** Обучить имплементированные модели (для классификации и регрессии)  \n",
    "**4.c** Оценить качество имплементированных моделей  \n",
    "**4.d** Сравнить результаты имплементированных моделей с пунктом 2  \n",
    "**4.e** Сделать выводы  \n",
    "**4.f** Добавить техники из улучшенного бейзлайна (пункт 3c)  \n",
    "**4.g** Обучить модели (для классификации и регрессии)  \n",
    "**4.h** Оценить качество моделей  \n",
    "**4.i** Сравнить результаты моделей в сравнении с пунктом 3  \n",
    "**4.j** Сделать выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Custom GB Regressor]\n",
      "MSE: 11.4839\n",
      "R^2: 0.4399\n",
      "\n",
      "[Custom GB Classifier]\n",
      "Accuracy: 0.6709\n",
      "F1-score: 0.8030\n"
     ]
    }
   ],
   "source": [
    "class SimpleDecisionStumpRegressor:\n",
    "    \"\"\"\n",
    "    Простейшая модель дерева глубины 1:\n",
    "    - один признак\n",
    "    - один порог\n",
    "    - предсказания = среднее слева/справа\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.feat = None\n",
    "        self.thresh = None\n",
    "        self.left_value = None\n",
    "        self.right_value = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Очень упрощённо: берём feat=0, thresh=median\n",
    "        # Вы можете расширить логику: искать лучший feat, лучший thresh\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        feat = 0\n",
    "        thresh = np.median(X[:, feat])\n",
    "        left_idx = X[:, feat] <= thresh\n",
    "        right_idx = ~left_idx\n",
    "\n",
    "        self.feat = feat\n",
    "        self.thresh = thresh\n",
    "        self.left_value = y[left_idx].mean() if len(y[left_idx]) > 0 else 0\n",
    "        self.right_value = y[right_idx].mean() if len(y[right_idx]) > 0 else 0\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        pred = np.zeros(X.shape[0])\n",
    "        left_idx = X[:, self.feat] <= self.thresh\n",
    "        right_idx = ~left_idx\n",
    "        pred[left_idx] = self.left_value\n",
    "        pred[right_idx] = self.right_value\n",
    "        return pred\n",
    "\n",
    "\n",
    "class SimpleGBRegressor:\n",
    "    \"\"\"\n",
    "    Упрощённый градиентный бустинг для регрессии:\n",
    "    - Инициализация: f0 = среднее целевой переменной\n",
    "    - Каждый новый базовый learner (decision stump) обучается на псевдо-остатках\n",
    "    - f_m(x) = f_{m-1}(x) + lr * h_m(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        self.f0 = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y, dtype=float)\n",
    "        \n",
    "        # step 0: f0 = среднее\n",
    "        self.f0 = np.mean(y)\n",
    "        \n",
    "        # инициализируем текущие предсказания\n",
    "        f_current = np.ones_like(y) * self.f0\n",
    "        \n",
    "        self.models = []\n",
    "        for i in range(self.n_estimators):\n",
    "            # псевдо-остатки\n",
    "            residuals = y - f_current\n",
    "            \n",
    "            # обучаем stump на residuals\n",
    "            stump = SimpleDecisionStumpRegressor()\n",
    "            stump.fit(X, residuals)\n",
    "            self.models.append(stump)\n",
    "            \n",
    "            # обновляем f_current\n",
    "            h_m = stump.predict(X)\n",
    "            f_current += self.learning_rate * h_m\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        # f0 + сумма(lr * h_m(x))\n",
    "        preds = np.ones(X.shape[0]) * self.f0\n",
    "        for stump in self.models:\n",
    "            preds += self.learning_rate * stump.predict(X)\n",
    "        return preds\n",
    "\n",
    "\n",
    "# ==================\n",
    "# Пример использования\n",
    "# ==================\n",
    "\n",
    "# 1) КАСТОМНАЯ РЕГРЕССИЯ\n",
    "cust_gb_reg = SimpleGBRegressor(n_estimators=10, learning_rate=0.1)\n",
    "cust_gb_reg.fit(Xr_train, yr_train)\n",
    "yr_pred_custom = cust_gb_reg.predict(Xr_test)\n",
    "\n",
    "mse_cust = mean_squared_error(yr_test, yr_pred_custom)\n",
    "r2_cust = r2_score(yr_test, yr_pred_custom)\n",
    "\n",
    "print(\"[Custom GB Regressor]\")\n",
    "print(f\"MSE: {mse_cust:.4f}\")\n",
    "print(f\"R^2: {r2_cust:.4f}\")\n",
    "\n",
    "# 2) Для классификации: можно сделать аналог (SimpleGBClassifier) \n",
    "#   - но обычно нужно считать логистическую ошибку / экспоненциальную и т.д.\n",
    "#   - здесь покажем упрощённый вариант на MSE, превращая метки 0/1 -> float.\n",
    "\n",
    "class SimpleGBClassifier:\n",
    "    \"\"\"\n",
    "    Упрощённо, будем считать 0/1, и \n",
    "    оптимизировать MSE между предсказанием и метками (как регрессия).\n",
    "    Потом threshold=0.5\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        self.f0 = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y, dtype=float)\n",
    "        self.f0 = np.mean(y)\n",
    "        f_current = np.ones_like(y) * self.f0\n",
    "        self.models = []\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            residuals = y - f_current\n",
    "            stump = SimpleDecisionStumpRegressor()\n",
    "            stump.fit(X, residuals)\n",
    "            self.models.append(stump)\n",
    "            f_current += self.learning_rate * stump.predict(X)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # f0 + сумма(lr * h_m(x)) -> sigmoid\n",
    "        raw_preds = np.ones(X.shape[0]) * self.f0\n",
    "        for stump in self.models:\n",
    "            raw_preds += self.learning_rate * stump.predict(X)\n",
    "        # применим сигмоиду\n",
    "        return 1 / (1 + np.exp(-raw_preds))\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba >= threshold).astype(int)\n",
    "\n",
    "\n",
    "cust_gb_clf = SimpleGBClassifier(n_estimators=10, learning_rate=0.1)\n",
    "cust_gb_clf.fit(Xc_train, yc_train)\n",
    "yc_pred_custom = cust_gb_clf.predict(Xc_test)\n",
    "\n",
    "acc_cust = accuracy_score(yc_test, yc_pred_custom)\n",
    "f1_cust = f1_score(yc_test, yc_pred_custom)\n",
    "\n",
    "print(\"\\n[Custom GB Classifier]\")\n",
    "print(f\"Accuracy: {acc_cust:.4f}\")\n",
    "print(f\"F1-score: {f1_cust:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
