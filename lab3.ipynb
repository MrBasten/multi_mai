{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №3 (Проведение исследований с решающим деревом)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание бейзлайна и оценка качества \n",
    "**2.a** Обучить модели из `sklearn` (для классификации и регрессии) для выбранных наборов данных  \n",
    "**2.b** Оценить качество моделей (для классификации и регрессии) по выбранным метрикам на выбранных наборах данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Бейзлайн DecisionTree (Classification):\n",
      "Accuracy:  0.8734\n",
      "Precision: 0.9574\n",
      "Recall:    0.8491\n",
      "F1-score:  0.9000\n",
      "\n",
      "Бейзлайн DecisionTree (Regression):\n",
      "MSE:  2.2500\n",
      "MAE:  1.0443\n",
      "R^2:  0.8903\n"
     ]
    }
   ],
   "source": [
    "# 1. Загрузка данных\n",
    "data = pd.read_csv('student_data.csv', sep=',')\n",
    "\n",
    "# 2. Классификация: задача \"сдал/не сдал\"\n",
    "data_class = data.copy()\n",
    "data_class['passed'] = (data_class['G3'] >= 10).astype(int)\n",
    "X_class = data_class[['G1', 'G2', 'studytime', 'failures', 'absences']]\n",
    "y_class = data_class['passed']\n",
    "\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
    ")\n",
    "\n",
    "# Бейзлайновая модель: DecisionTreeClassifier без особых настроек\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(Xc_train, yc_train)\n",
    "yc_pred = dt_clf.predict(Xc_test)\n",
    "\n",
    "acc = accuracy_score(yc_test, yc_pred)\n",
    "prec = precision_score(yc_test, yc_pred)\n",
    "rec = recall_score(yc_test, yc_pred)\n",
    "f1 = f1_score(yc_test, yc_pred)\n",
    "\n",
    "print(\"Бейзлайн DecisionTree (Classification):\")\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")\n",
    "\n",
    "# 3. Регрессия: предсказание оценки G3\n",
    "X_reg = data[['G1', 'G2', 'studytime', 'failures', 'absences']]\n",
    "y_reg = data['G3']\n",
    "\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Бейзлайновая модель: DecisionTreeRegressor без особых настроек\n",
    "dt_reg = DecisionTreeRegressor(random_state=42)\n",
    "dt_reg.fit(Xr_train, yr_train)\n",
    "yr_pred = dt_reg.predict(Xr_test)\n",
    "\n",
    "mse = mean_squared_error(yr_test, yr_pred)\n",
    "mae = mean_absolute_error(yr_test, yr_pred)\n",
    "r2 = r2_score(yr_test, yr_pred)\n",
    "\n",
    "print(\"\\nБейзлайн DecisionTree (Regression):\")\n",
    "print(f\"MSE:  {mse:.4f}\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"R^2:  {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Улучшение бейзлайна\n",
    "**3.a** Сформулировать гипотезы (препроцессинг, визуализация, новые признаки, подбор гиперпараметров и т.д.)  \n",
    "**3.b** Проверить гипотезы  \n",
    "**3.c** Сформировать улучшенный бейзлайн  \n",
    "**3.d** Обучить модели с улучшенным бейзлайном (для классификации и регрессии)  \n",
    "**3.e** Оценить качество моделей  \n",
    "**3.f** Сравнить результаты с пунктом 2  \n",
    "**3.g** Сделать выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Улучшенный бейзлайн (DecisionTree Classifier):\n",
      "Лучшие параметры: {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 10}\n",
      "Accuracy:  0.8861\n",
      "Precision: 0.9583\n",
      "Recall:    0.8679\n",
      "F1-score:  0.9109\n",
      "Shape after dropna: (395, 33)\n",
      "Train size: (313, 18), Test size: (79, 18)\n",
      "Лучшие параметры: {'dt__criterion': 'squared_error', 'dt__max_depth': 3, 'dt__min_samples_leaf': 1, 'dt__min_samples_split': 2}\n",
      "\n",
      "=== Улучшенный бейзлайн DecisionTree (Regression) ===\n",
      "MSE:  3.1542\n",
      "MAE:  1.1418\n",
      "R^2:  0.7744\n"
     ]
    }
   ],
   "source": [
    "# Пример гипотезы улучшения:\n",
    "# 1) Подбор гиперпараметров решающего дерева (max_depth, min_samples_split, min_samples_leaf...)\n",
    "# 2) Масштабирование признаков, если считаем нужным (хотя дерево не так критично реагирует)\n",
    "# 3) Добавление новых признаков (например, total_G1_G2 = G1 + G2)\n",
    "# 4) Удаление выбросов (например, absences > 40)\n",
    "\n",
    "# Для демонстрации: создадим признак total_G1_G2\n",
    "data_class['total_G1_G2'] = data_class['G1'] + data_class['G2']\n",
    "Xc_train2 = data_class.loc[Xc_train.index, ['G1','G2','studytime','failures','absences','total_G1_G2']]\n",
    "Xc_test2  = data_class.loc[Xc_test.index,  ['G1','G2','studytime','failures','absences','total_G1_G2']]\n",
    "\n",
    "# 3.1 Улучшение для классификации\n",
    "param_grid_clf = {\n",
    "    'max_depth': [3, 5, 7, None],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'criterion': ['gini','entropy']\n",
    "}\n",
    "clf_cv = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
    "                      param_grid_clf, cv=5, scoring='f1', n_jobs=-1)\n",
    "clf_cv.fit(Xc_train2, yc_train)\n",
    "\n",
    "best_clf = clf_cv.best_estimator_\n",
    "yc_pred_best = best_clf.predict(Xc_test2)\n",
    "\n",
    "acc_best = accuracy_score(yc_test, yc_pred_best)\n",
    "prec_best = precision_score(yc_test, yc_pred_best)\n",
    "rec_best = recall_score(yc_test, yc_pred_best)\n",
    "f1_best = f1_score(yc_test, yc_pred_best)\n",
    "\n",
    "print(\"Улучшенный бейзлайн (DecisionTree Classifier):\")\n",
    "print(\"Лучшие параметры:\", clf_cv.best_params_)\n",
    "print(f\"Accuracy:  {acc_best:.4f}\")\n",
    "print(f\"Precision: {prec_best:.4f}\")\n",
    "print(f\"Recall:    {rec_best:.4f}\")\n",
    "print(f\"F1-score:  {f1_best:.4f}\")\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "print(\"Shape after dropna:\", data.shape)\n",
    "data = data[data['absences'] <= 40]\n",
    "categorical_cols = ['Mjob', 'Fjob', 'reason', 'guardian']\n",
    "binary_cols = ['schoolsup', 'famsup', 'paid', 'activities', 'nursery', \n",
    "               'higher', 'internet', 'romantic']\n",
    "\n",
    "# Преобразуем yes/no -> 1/0\n",
    "for col in binary_cols:\n",
    "    data[col] = data[col].map({'yes':1, 'no':0})\n",
    "data['total_G1_G2'] = data['G1'] + data['G2']\n",
    "\n",
    "numeric_cols = ['G1', 'G2', 'studytime', 'failures', 'absences', 'total_G1_G2'] + binary_cols\n",
    "\n",
    "X = data[numeric_cols + categorical_cols].copy()\n",
    "y = data['G3'].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train size: {X_train.shape}, Test size: {X_test.shape}\")\n",
    "\n",
    "\n",
    "numeric_transformer = StandardScaler()  # Можно и не масштабировать — иногда не нужно для деревьев\n",
    "cat_transformer = OneHotEncoder(drop='first', handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, numeric_cols),\n",
    "    (\"cat\", cat_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "# 7.2 Пайплайн: сперва preprocessor -> потом DecisionTreeRegressor\n",
    "pipe_dt = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"dt\", DecisionTreeRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# =========================\n",
    "# 8. ПОДБОР ГИПЕРПАРАМЕТРОВ\n",
    "# =========================\n",
    "param_grid = {\n",
    "    \"dt__max_depth\": [3, 5, 7, 10, None],\n",
    "    \"dt__min_samples_leaf\": [1, 2, 5, 10],\n",
    "    \"dt__min_samples_split\": [2, 5, 10],\n",
    "    \"dt__criterion\": [\"squared_error\", \"absolute_error\"]\n",
    "    # Можно добавить/убрать любые параметры\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipe_dt,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "\n",
    "# =========================\n",
    "# 9. ОЦЕНКА КАЧЕСТВА УЛУЧШЕННОГО БЕЙЗЛАЙНА\n",
    "# =========================\n",
    "best_dt = grid_search.best_estimator_\n",
    "y_pred = best_dt.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n=== Улучшенный бейзлайн DecisionTree (Regression) ===\")\n",
    "print(f\"MSE:  {mse:.4f}\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"R^2:  {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Имплементация алгоритма машинного обучения (Random Forest)\n",
    "\n",
    "**4.a** Самостоятельно имплементировать алгоритмы (для классификации и регрессии)  \n",
    "**4.b** Обучить имплементированные модели (для классификации и регрессии)  \n",
    "**4.c** Оценить качество имплементированных моделей  \n",
    "**4.d** Сравнить результаты имплементированных моделей с пунктом 2  \n",
    "**4.e** Сделать выводы  \n",
    "**4.f** Добавить техники из улучшенного бейзлайна (пункт 3.c)  \n",
    "**4.g** Обучить модели (для классификации и регрессии)  \n",
    "**4.h** Оценить качество моделей  \n",
    "**4.i** Сравнить результаты моделей с пунктом 3  \n",
    "**4.j** Сделать выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Decision Tree - Classification:\n",
      "Accuracy: 0.7848\n",
      "F1-score: 0.8247\n",
      "\n",
      "Custom Decision Tree - Regression:\n",
      "MSE: 6.1922\n",
      "R^2: 0.6980\n"
     ]
    }
   ],
   "source": [
    "# Ячейка [4] (Code)\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "########################################################################\n",
    "# Пример упрощенной имплементации дерева \"с нуля\" (для демонстрации)\n",
    "# В реальной практике это намного более объёмный код \n",
    "# и обычно используют готовые библиотеки.\n",
    "########################################################################\n",
    "\n",
    "class CustomDecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        self.root = self._build_tree(X, y, depth=0)\n",
    "        return self\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        # Простейшая рекурсия; неточная реализация\n",
    "        # 1) критерий остановки\n",
    "        if len(set(y)) == 1:\n",
    "            return {'type':'leaf','class':y[0]}\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            # Мажоритарный класс\n",
    "            return {'type':'leaf','class': Counter(y).most_common(1)[0][0]}\n",
    "        if len(X) < self.min_samples_split:\n",
    "            return {'type':'leaf','class': Counter(y).most_common(1)[0][0]}\n",
    "\n",
    "        # 2) Выбираем лучший сплит (очень упрощённый)\n",
    "        best_feat = 0\n",
    "        best_thresh = np.median(X[:,0])\n",
    "        # Можно реализовать поиск по всем признакам и \n",
    "        # разным значениям, считаем прирост информации, Gini, и т.д.\n",
    "\n",
    "        # 3) Делим выборку\n",
    "        left_idx = X[:,best_feat] <= best_thresh\n",
    "        right_idx = ~left_idx\n",
    "\n",
    "        # 4) Построение узлов\n",
    "        node = {\n",
    "            'type': 'node',\n",
    "            'feature': best_feat,\n",
    "            'thresh': best_thresh,\n",
    "            'left':  self._build_tree(X[left_idx], y[left_idx], depth+1),\n",
    "            'right': self._build_tree(X[right_idx], y[right_idx], depth+1)\n",
    "        }\n",
    "        return node\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        preds = []\n",
    "        for x in X:\n",
    "            preds.append(self._traverse(self.root, x))\n",
    "        return np.array(preds)\n",
    "\n",
    "    def _traverse(self, node, x):\n",
    "        if node['type'] == 'leaf':\n",
    "            return node['class']\n",
    "        else:\n",
    "            if x[node['feature']] <= node['thresh']:\n",
    "                return self._traverse(node['left'], x)\n",
    "            else:\n",
    "                return self._traverse(node['right'], x)\n",
    "\n",
    "\n",
    "class CustomDecisionTreeRegressor:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        self.root = self._build_tree(X, y, depth=0)\n",
    "        return self\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return {'type':'leaf', 'value': y.mean()}\n",
    "        if len(X) < self.min_samples_split:\n",
    "            return {'type':'leaf', 'value': y.mean()}\n",
    "        if np.allclose(y, y[0]):\n",
    "            return {'type':'leaf', 'value': y[0]}\n",
    "\n",
    "        # Упрощённый сплит - берем первый признак\n",
    "        best_feat = 0\n",
    "        best_thresh = np.median(X[:,0])\n",
    "\n",
    "        left_idx = X[:,best_feat] <= best_thresh\n",
    "        right_idx = ~left_idx\n",
    "\n",
    "        node = {\n",
    "            'type': 'node',\n",
    "            'feature': best_feat,\n",
    "            'thresh': best_thresh,\n",
    "            'left': self._build_tree(X[left_idx], y[left_idx], depth+1),\n",
    "            'right': self._build_tree(X[right_idx], y[right_idx], depth+1)\n",
    "        }\n",
    "        return node\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        preds = []\n",
    "        for x in X:\n",
    "            preds.append(self._traverse(self.root, x))\n",
    "        return np.array(preds)\n",
    "\n",
    "    def _traverse(self, node, x):\n",
    "        if node['type'] == 'leaf':\n",
    "            return node['value']\n",
    "        else:\n",
    "            if x[node['feature']] <= node['thresh']:\n",
    "                return self._traverse(node['left'], x)\n",
    "            else:\n",
    "                return self._traverse(node['right'], x)\n",
    "\n",
    "\n",
    "# 4.a-b Обучим кастомные модели\n",
    "# Для классификации\n",
    "cust_clf = CustomDecisionTreeClassifier(max_depth=3, min_samples_split=5)\n",
    "cust_clf.fit(Xc_train, yc_train)\n",
    "yc_pred_custom = cust_clf.predict(Xc_test)\n",
    "\n",
    "acc_cust = accuracy_score(yc_test, yc_pred_custom)\n",
    "f1_cust = f1_score(yc_test, yc_pred_custom)\n",
    "\n",
    "print(\"Custom Decision Tree - Classification:\")\n",
    "print(f\"Accuracy: {acc_cust:.4f}\")\n",
    "print(f\"F1-score: {f1_cust:.4f}\")\n",
    "\n",
    "# Для регрессии\n",
    "cust_reg = CustomDecisionTreeRegressor(max_depth=3, min_samples_split=5)\n",
    "cust_reg.fit(Xr_train, yr_train)\n",
    "yr_pred_custom = cust_reg.predict(Xr_test)\n",
    "\n",
    "mse_cust = mean_squared_error(yr_test, yr_pred_custom)\n",
    "r2_cust = r2_score(yr_test, yr_pred_custom)\n",
    "\n",
    "print(\"\\nCustom Decision Tree - Regression:\")\n",
    "print(f\"MSE: {mse_cust:.4f}\")\n",
    "print(f\"R^2: {r2_cust:.4f}\")\n",
    "\n",
    "# 4.c - Оценить качество (см. выше)\n",
    "# 4.d - Сравнить с пунктом 2 (было dt_clf, dt_reg из sklearn)\n",
    "# 4.e - Выводы\n",
    "\n",
    "# 4.f-g - Применить улучшения (например, меняем параметры, добавляем новые фичи)\n",
    "# ...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
