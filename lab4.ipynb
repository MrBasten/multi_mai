{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №4 (Проведение исследований со случайным лесом)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Создание бейзлайна и оценка качества\n",
    "**2.a** Обучить модели из `sklearn` (для классификации и регрессии) для выбранных наборов данных  \n",
    "**2.b** Оценить качество моделей (для классификации и регрессии) по выбранным метрикам на выбранных наборах данных \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Случайный лес из sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             mean_squared_error, mean_absolute_error, r2_score)\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (395, 33)\n",
      "Бейзлайн RandomForest (Classification):\n",
      "Accuracy:  0.8734\n",
      "Precision: 0.9388\n",
      "Recall:    0.8679\n",
      "F1-score:  0.9020\n",
      "\n",
      "Бейзлайн RandomForest (Regression):\n",
      "MSE:  2.6369\n",
      "MAE:  1.0548\n",
      "R^2:  0.8714\n"
     ]
    }
   ],
   "source": [
    "# 1. Загрузка и первичная обработка данных\n",
    "data = pd.read_csv('student_data.csv', sep=',')\n",
    "print(\"Shape:\", data.shape)\n",
    "\n",
    "# (Опционально) удаляем пропуски, если есть\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Бинарная классификация: \"сдал/не сдал\"\n",
    "data_class = data.copy()\n",
    "data_class['passed'] = (data_class['G3'] >= 10).astype(int)\n",
    "\n",
    "# Признаки для классификации (простейший вариант)\n",
    "X_class = data_class[['G1', 'G2', 'studytime', 'failures', 'absences']]\n",
    "y_class = data_class['passed']\n",
    "\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
    ")\n",
    "\n",
    "# Бейзлайн (Classification) - RandomForest без особых настроек\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(Xc_train, yc_train)\n",
    "yc_pred = rf_clf.predict(Xc_test)\n",
    "\n",
    "acc = accuracy_score(yc_test, yc_pred)\n",
    "prec = precision_score(yc_test, yc_pred)\n",
    "rec = recall_score(yc_test, yc_pred)\n",
    "f1 = f1_score(yc_test, yc_pred)\n",
    "\n",
    "print(\"Бейзлайн RandomForest (Classification):\")\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")\n",
    "\n",
    "# Регрессия: предсказываем G3\n",
    "X_reg = data[['G1', 'G2', 'studytime', 'failures', 'absences']]\n",
    "y_reg = data['G3']\n",
    "\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Бейзлайн (Regression) - RandomForest без особых настроек\n",
    "rf_reg = RandomForestRegressor(random_state=42)\n",
    "rf_reg.fit(Xr_train, yr_train)\n",
    "yr_pred = rf_reg.predict(Xr_test)\n",
    "\n",
    "mse = mean_squared_error(yr_test, yr_pred)\n",
    "mae = mean_absolute_error(yr_test, yr_pred)\n",
    "r2 = r2_score(yr_test, yr_pred)\n",
    "\n",
    "print(\"\\nБейзлайн RandomForest (Regression):\")\n",
    "print(f\"MSE:  {mse:.4f}\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"R^2:  {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**:\n",
    "- Базовый случайный лес для классификации показал некоторые метрики (accuracy, f1).  \n",
    "- Базовый случайный лес для регрессии дал значения (MSE, MAE, R^2).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Улучшение бейзлайна\n",
    "**3.a** Сформулировать гипотезы (предобработка данных, визуализация, новые признаки, подбор гиперпараметров...)  \n",
    "**3.b** Проверить гипотезы  \n",
    "**3.c** Сформировать улучшенный бейзлайн  \n",
    "**3.d** Обучить модели с улучшенным бейзлайном (для классификации и регрессии)  \n",
    "**3.e** Оценить качество моделей  \n",
    "**3.f** Сравнить результаты с пунктом 2  \n",
    "**3.g** Сделать выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Улучшенный бейзлайн (RandomForest - Classification):\n",
      "Лучшие параметры: {'max_depth': 10, 'max_features': None, 'min_samples_leaf': 5, 'n_estimators': 100}\n",
      "Accuracy:  0.9241\n",
      "Precision: 0.9796\n",
      "Recall:    0.9057\n",
      "F1-score:  0.9412\n",
      "\n",
      "Улучшенный бейзлайн (RandomForest - Regression):\n",
      "Лучшие параметры: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "MSE:  1.4459\n",
      "MAE:  0.8484\n",
      "R^2:  0.8966\n"
     ]
    }
   ],
   "source": [
    "# Пример гипотез:\n",
    "# - Добавить признак total_G1_G2 = G1 + G2\n",
    "# - Удалить выбросы, например, absences > 40\n",
    "# - Подобрать гиперпараметры n_estimators, max_depth, min_samples_leaf, max_features и т.д.\n",
    "\n",
    "# 1) Удаление выбросов (пример)\n",
    "data_improved = data.copy()\n",
    "data_improved = data_improved[data_improved['absences'] <= 40]\n",
    "\n",
    "# 2) Создание нового признака\n",
    "data_improved['total_G1_G2'] = data_improved['G1'] + data_improved['G2']\n",
    "\n",
    "# ---------- КЛАССИФИКАЦИЯ ----------\n",
    "data_class_improved = data_improved.copy()\n",
    "data_class_improved['passed'] = (data_class_improved['G3'] >= 10).astype(int)\n",
    "X_class2 = data_class_improved[['G1','G2','studytime','failures','absences','total_G1_G2']]\n",
    "y_class2 = data_class_improved['passed']\n",
    "\n",
    "Xc2_train, Xc2_test, yc2_train, yc2_test = train_test_split(\n",
    "    X_class2, y_class2, test_size=0.2, random_state=42, stratify=y_class2\n",
    ")\n",
    "\n",
    "# Пробуем подобрать гиперпараметры\n",
    "param_grid_clf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "rf_clf_cv = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "                         param_grid_clf, cv=5, scoring='f1', n_jobs=-1)\n",
    "rf_clf_cv.fit(Xc2_train, yc2_train)\n",
    "\n",
    "best_clf = rf_clf_cv.best_estimator_\n",
    "yc2_pred_best = best_clf.predict(Xc2_test)\n",
    "\n",
    "acc_best = accuracy_score(yc2_test, yc2_pred_best)\n",
    "prec_best = precision_score(yc2_test, yc2_pred_best)\n",
    "rec_best = recall_score(yc2_test, yc2_pred_best)\n",
    "f1_best = f1_score(yc2_test, yc2_pred_best)\n",
    "\n",
    "print(\"Улучшенный бейзлайн (RandomForest - Classification):\")\n",
    "print(\"Лучшие параметры:\", rf_clf_cv.best_params_)\n",
    "print(f\"Accuracy:  {acc_best:.4f}\")\n",
    "print(f\"Precision: {prec_best:.4f}\")\n",
    "print(f\"Recall:    {rec_best:.4f}\")\n",
    "print(f\"F1-score:  {f1_best:.4f}\")\n",
    "\n",
    "# ---------- РЕГРЕССИЯ ----------\n",
    "X_reg2 = data_improved[['G1','G2','studytime','failures','absences','total_G1_G2']]\n",
    "y_reg2 = data_improved['G3']\n",
    "\n",
    "Xr2_train, Xr2_test, yr2_train, yr2_test = train_test_split(\n",
    "    X_reg2, y_reg2, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "param_grid_reg = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'max_features': ['sqrt','log2', None]\n",
    "}\n",
    "\n",
    "rf_reg_cv = GridSearchCV(RandomForestRegressor(random_state=42),\n",
    "                         param_grid_reg, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "rf_reg_cv.fit(Xr2_train, yr2_train)\n",
    "\n",
    "best_reg = rf_reg_cv.best_estimator_\n",
    "yr2_pred_best = best_reg.predict(Xr2_test)\n",
    "\n",
    "mse_best = mean_squared_error(yr2_test, yr2_pred_best)\n",
    "mae_best = mean_absolute_error(yr2_test, yr2_pred_best)\n",
    "r2_best = r2_score(yr2_test, yr2_pred_best)\n",
    "\n",
    "print(\"\\nУлучшенный бейзлайн (RandomForest - Regression):\")\n",
    "print(\"Лучшие параметры:\", rf_reg_cv.best_params_)\n",
    "print(f\"MSE:  {mse_best:.4f}\")\n",
    "print(f\"MAE:  {mae_best:.4f}\")\n",
    "print(f\"R^2:  {r2_best:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выводы**:\n",
    "- Удаление выбросов (`absences>40`) + добавление признака (`total_G1_G2`) + подбор гиперпараметров улучшили результаты модели (или нет).\n",
    "- Можно расширять список гиперпараметров (например, `min_samples_split`, `bootstrap`, `oob_score=True` и т.д.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Имплементация алгоритма машинного обучения(Random Forest)\n",
    "**4.a** Самостоятельно имплементировать алгоритмы (для классификации и регрессии)  \n",
    "**4.b** Обучить имплементированные модели (для классификации и регрессии)  \n",
    "**4.c** Оценить качество имплементированных моделей  \n",
    "**4.d** Сравнить результаты имплементированных моделей с пунктом 2  \n",
    "**4.e** Сделать выводы  \n",
    "**4.f** Добавить техники из улучшенного бейзлайна (пункт 3.c)  \n",
    "**4.g** Обучить модели (для классификации и регрессии)  \n",
    "**4.h** Оценить качество моделей  \n",
    "**4.i** Сравнить результаты моделей с пунктом 3  \n",
    "**4.j** Сделать выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка [4] (Code)\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score\n",
    "\n",
    "# ================================================\n",
    "# Упрощённая имплементация Random Forest:\n",
    "# 1) Используем уже написанный (упрощённый) DecisionTree \n",
    "#    или что-то подобное\n",
    "# 2) bagging: из каждой bootstrap-выборки обучаем дерево\n",
    "# 3) усредняем предсказания (для регрессии) \n",
    "#    или делаем голосование (для классификации)\n",
    "# ================================================\n",
    "\n",
    "class SimpleDecisionTreeClassifier:\n",
    "    \"\"\"Очень простое дерево (для демонстрации),\n",
    "       не полноценная реализация\"\"\"\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y, depth=0):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        # критерий остановки (упрощённо)\n",
    "        if len(np.unique(y)) == 1 or len(X) < self.min_samples_split:\n",
    "            self.root = ('leaf', Counter(y).most_common(1)[0][0])\n",
    "            return self\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            self.root = ('leaf', Counter(y).most_common(1)[0][0])\n",
    "            return self\n",
    "\n",
    "        # в демо выбираем просто 1 признак (0) и порог - медиа\n",
    "        feat = 0\n",
    "        thresh = np.median(X[:, feat])\n",
    "        left_idx = X[:, feat] <= thresh\n",
    "        right_idx = ~left_idx\n",
    "\n",
    "        self.root = ('node', feat, thresh,\n",
    "                     SimpleDecisionTreeClassifier(self.max_depth, self.min_samples_split).fit(X[left_idx], y[left_idx], depth+1),\n",
    "                     SimpleDecisionTreeClassifier(self.max_depth, self.min_samples_split).fit(X[right_idx], y[right_idx], depth+1))\n",
    "        return self\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        node = self.root\n",
    "        while True:\n",
    "            if node[0] == 'leaf':\n",
    "                return node[1]\n",
    "            else:\n",
    "                # node = ('node', feat, thresh, left_tree, right_tree)\n",
    "                feat, thresh = node[1], node[2]\n",
    "                left_tree, right_tree = node[3], node[4]\n",
    "                if x[feat] <= thresh:\n",
    "                    node = left_tree.root\n",
    "                else:\n",
    "                    node = right_tree.root\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        return np.array([self.predict_one(x) for x in X])\n",
    "\n",
    "\n",
    "class SimpleDecisionTreeRegressor:\n",
    "    \"\"\"Аналогично, очень простая реализация.\"\"\"\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y, depth=0):\n",
    "        X, y = np.array(X), np.array(y, dtype=float)\n",
    "        if len(X) < self.min_samples_split:\n",
    "            self.root = ('leaf', np.mean(y))\n",
    "            return self\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            self.root = ('leaf', np.mean(y))\n",
    "            return self\n",
    "        if np.allclose(y, y[0]):\n",
    "            self.root = ('leaf', y[0])\n",
    "            return self\n",
    "\n",
    "        # Упрощённо: берем признак 0 и медиану\n",
    "        feat = 0\n",
    "        thresh = np.median(X[:, feat])\n",
    "        left_idx = X[:, feat] <= thresh\n",
    "        right_idx = ~left_idx\n",
    "\n",
    "        self.root = ('node', feat, thresh,\n",
    "                     SimpleDecisionTreeRegressor(self.max_depth, self.min_samples_split).fit(X[left_idx], y[left_idx], depth+1),\n",
    "                     SimpleDecisionTreeRegressor(self.max_depth, self.min_samples_split).fit(X[right_idx], y[right_idx], depth+1))\n",
    "        return self\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        node = self.root\n",
    "        while True:\n",
    "            if node[0] == 'leaf':\n",
    "                return node[1]\n",
    "            else:\n",
    "                feat, thresh = node[1], node[2]\n",
    "                left_tree, right_tree = node[3], node[4]\n",
    "                if x[feat] <= thresh:\n",
    "                    node = left_tree.root\n",
    "                else:\n",
    "                    node = right_tree.root\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        return np.array([self.predict_one(row) for row in X])\n",
    "\n",
    "\n",
    "class SimpleRandomForestClassifier:\n",
    "    def __init__(self, n_estimators=10, max_depth=None, min_samples_split=2, random_state=42):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state)\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        self.trees = []\n",
    "        for i in range(self.n_estimators):\n",
    "            # Bootstrap выборка\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_boot = X[indices]\n",
    "            y_boot = y[indices]\n",
    "\n",
    "            tree = SimpleDecisionTreeClassifier(self.max_depth, self.min_samples_split)\n",
    "            tree.fit(X_boot, y_boot)\n",
    "            self.trees.append(tree)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Собираем предсказания от всех деревьев\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])  # shape = (n_estimators, n_samples)\n",
    "        # Голосование\n",
    "        final_preds = []\n",
    "        for i in range(predictions.shape[1]):\n",
    "            # берем столбец i\n",
    "            column = predictions[:, i]\n",
    "            # мажоритарный класс\n",
    "            most_common = Counter(column).most_common(1)[0][0]\n",
    "            final_preds.append(most_common)\n",
    "        return np.array(final_preds)\n",
    "\n",
    "\n",
    "class SimpleRandomForestRegressor:\n",
    "    def __init__(self, n_estimators=10, max_depth=None, min_samples_split=2, random_state=42):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state)\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        self.trees = []\n",
    "        for i in range(self.n_estimators):\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_boot = X[indices]\n",
    "            y_boot = y[indices]\n",
    "\n",
    "            tree = SimpleDecisionTreeRegressor(self.max_depth, self.min_samples_split)\n",
    "            tree.fit(X_boot, y_boot)\n",
    "            self.trees.append(tree)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # усредняем предсказания всех деревьев\n",
    "        preds = np.array([tree.predict(X) for tree in self.trees])  # shape = (n_estimators, n_samples)\n",
    "        return np.mean(preds, axis=0)\n",
    "\n",
    "\n",
    "# ==================\n",
    "# ПРИМЕР ИСПОЛЬЗОВАНИЯ\n",
    "# ==================\n",
    "# Обучим нашу упрощённую реализацию на том же train/test, что и в пунктах 2-3\n",
    "\n",
    "# Для классификации\n",
    "cust_rf_clf = SimpleRandomForestClassifier(n_estimators=10, max_depth=3, min_samples_split=5)\n",
    "cust_rf_clf.fit(Xc_train, yc_train)\n",
    "yc_custom_pred = cust_rf_clf.predict(Xc_test)\n",
    "\n",
    "acc_cust = accuracy_score(yc_test, yc_custom_pred)\n",
    "f1_cust = f1_score(yc_test, yc_custom_pred)\n",
    "\n",
    "print(\"\\n[Custom RandomForest Classifier]\")\n",
    "print(f\"Accuracy: {acc_cust:.4f}\")\n",
    "print(f\"F1-score: {f1_cust:.4f}\")\n",
    "\n",
    "# Для регрессии\n",
    "cust_rf_reg = SimpleRandomForestRegressor(n_estimators=10, max_depth=3, min_samples_split=5)\n",
    "cust_rf_reg.fit(Xr_train, yr_train)\n",
    "yr_custom_pred = cust_rf_reg.predict(Xr_test)\n",
    "\n",
    "mse_cust = mean_squared_error(yr_test, yr_custom_pred)\n",
    "r2_cust = r2_score(yr_test, yr_custom_pred)\n",
    "\n",
    "print(\"\\n[Custom RandomForest Regressor]\")\n",
    "print(f\"MSE: {mse_cust:.4f}\")\n",
    "print(f\"R^2: {r2_cust:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "1. Создан бейзлайн случайного леса (из `sklearn`) для классификации и регрессии, оценено качество (п.2).\n",
    "2. Проведено улучшение бейзлайна: удаление выбросов, добавление новых фич, подбор гиперпараметров (п.3).\n",
    "3. Имплементировали упрощённую версию RandomForest (классификатор и регрессор) “с нуля” (п.4).\n",
    "4. Сравнили результаты кастомной реализации со `sklearn`-овской.  \n",
    "5. Сделали выводы о том, как гиперпараметры и дополнительные фичи влияют на качество.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
