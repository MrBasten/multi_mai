{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №2 (Проведение исследований с логистической и линейной регрессией)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Создание бейзлайна и оценка качества\n",
    "\n",
    "**2.a** Обучить модели из sklearn (для классификации и регрессии) для выбранных наборов данных  \n",
    "**2.b** Оценить качество моделей (для классификации и регрессии) по выбранным метрикам на выбранных наборах данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Бейзлайн (Logistic Regression) - Classification:\n",
      "Accuracy:  0.8734\n",
      "Precision: 0.9388\n",
      "Recall:    0.8679\n",
      "F1-score:  0.9020\n",
      "\n",
      "Бейзлайн (Linear Regression) - Regression:\n",
      "MSE:  4.4665\n",
      "MAE:  1.3394\n",
      "R^2:  0.7822\n"
     ]
    }
   ],
   "source": [
    "# 1. Загрузка данных\n",
    "data = pd.read_csv('student_data.csv', sep=',')\n",
    "\n",
    "data_class = data.copy()\n",
    "data_class['passed'] = (data_class['G3'] >= 10).astype(int)\n",
    "X_class = data_class[['G1', 'G2', 'studytime', 'failures', 'absences']]\n",
    "y_class = data_class['passed']\n",
    "\n",
    "# Разделим на train/test\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
    ")\n",
    "\n",
    "# 3. Бейзлайн для классификации (Logistic Regression)\n",
    "logreg_baseline = LogisticRegression(max_iter=200, random_state=42)\n",
    "logreg_baseline.fit(Xc_train, yc_train)\n",
    "yc_pred_baseline = logreg_baseline.predict(Xc_test)\n",
    "\n",
    "acc = accuracy_score(yc_test, yc_pred_baseline)\n",
    "prec = precision_score(yc_test, yc_pred_baseline)\n",
    "rec = recall_score(yc_test, yc_pred_baseline)\n",
    "f1 = f1_score(yc_test, yc_pred_baseline)\n",
    "\n",
    "print(\"Бейзлайн (Logistic Regression) - Classification:\")\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")\n",
    "\n",
    "# 4. Регрессия: Предсказываем G3\n",
    "X_reg = data[['G1', 'G2', 'studytime', 'failures', 'absences']]\n",
    "y_reg = data['G3']\n",
    "\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Бейзлайн для регрессии (Linear Regression)\n",
    "linreg_baseline = LinearRegression()\n",
    "linreg_baseline.fit(Xr_train, yr_train)\n",
    "yr_pred_baseline = linreg_baseline.predict(Xr_test)\n",
    "\n",
    "mse = mean_squared_error(yr_test, yr_pred_baseline)\n",
    "mae = mean_absolute_error(yr_test, yr_pred_baseline)\n",
    "r2 = r2_score(yr_test, yr_pred_baseline)\n",
    "\n",
    "print(\"\\nБейзлайн (Linear Regression) - Regression:\")\n",
    "print(f\"MSE:  {mse:.4f}\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"R^2:  {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**:  \n",
    "- Логистическая регрессия даёт определённые метрики (accuracy, precision, recall, f1) для задачи \"сдал/не сдал\".  \n",
    "- Линейная регрессия даёт базовые результаты MSE, MAE, R^2 для предсказания итоговой оценки G3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Улучшение бейзлайна\n",
    "**3.a** Сформулировать гипотезы (предобработка данных, визуализация, новые признаки, подбор гиперпараметров и т.д.)  \n",
    "**3.b** Проверить гипотезы  \n",
    "**3.c** Сформировать улучшенный бейзлайн  \n",
    "**3.d** Обучить модели с улучшенным бейзлайном (логистическая и линейная регрессия)  \n",
    "**3.e** Оценить качество моделей с улучшенным бейзлайном  \n",
    "**3.f** Сравнить результаты моделей с улучшенным бейзлайном с пунктом 2  \n",
    "**3.g** Сделать выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Улучшенный бейзлайн (Логист. регрессия) =====\n",
      "Лучшие параметры: {'logreg__C': 1, 'logreg__penalty': 'l1', 'scaler': MinMaxScaler()}\n",
      "Accuracy:  0.8608\n",
      "Precision: 0.9038\n",
      "Recall:    0.8868\n",
      "F1-score:  0.8952\n",
      "\n",
      "===== Улучшенный бейзлайн (Ridge регрессия) =====\n",
      "Лучшие параметры: {'poly__degree': 2, 'ridge__alpha': 0.1, 'scaler': MinMaxScaler()}\n",
      "MSE:  2.4281\n",
      "MAE:  1.1251\n",
      "R^2:  0.8264\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# ЧАСТЬ КОДА УЛУЧШЕНИЯ БЕЙЗЛАЙНА\n",
    "# ====================================\n",
    "\n",
    "# Удалим пропуски (если они есть; если нет - этот шаг пропустится)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Пример удаления выбросов по признаку 'absences' (здесь порог 40 как пример)\n",
    "data = data[data['absences'] <= 40]\n",
    "\n",
    "# Создадим новый признак total_G1_G2\n",
    "data['total_G1_G2'] = data['G1'] + data['G2']\n",
    "\n",
    "# ----- (2) УЛУЧШЕНИЕ ДЛЯ КЛАССИФИКАЦИИ: ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ -----\n",
    "\n",
    "# Целевая переменная (сдал / не сдал)\n",
    "data_class = data.copy()\n",
    "data_class['passed'] = (data_class['G3'] >= 10).astype(int)\n",
    "X_class = data_class[['G1', 'G2', 'studytime', 'failures', 'absences', 'total_G1_G2']]\n",
    "y_class = data_class['passed']\n",
    "\n",
    "# Разделяем\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
    ")\n",
    "\n",
    "# Пайплайн для улучшенной модели: масштабирование + лог. регрессия\n",
    "pipe_logreg = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Можно менять на MinMaxScaler, RobustScaler\n",
    "    ('logreg', LogisticRegression(solver='saga', max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Сетка гиперпараметров\n",
    "param_grid_logreg = {\n",
    "    'scaler': [StandardScaler(), MinMaxScaler(), RobustScaler()],\n",
    "    'logreg__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'logreg__penalty': ['l1','l2']\n",
    "}\n",
    "\n",
    "# GridSearch\n",
    "grid_logreg = GridSearchCV(pipe_logreg, param_grid_logreg, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid_logreg.fit(Xc_train, yc_train)\n",
    "\n",
    "best_logreg = grid_logreg.best_estimator_\n",
    "yc_pred = best_logreg.predict(Xc_test)\n",
    "\n",
    "acc_best = accuracy_score(yc_test, yc_pred)\n",
    "prec_best = precision_score(yc_test, yc_pred)\n",
    "rec_best = recall_score(yc_test, yc_pred)\n",
    "f1_best = f1_score(yc_test, yc_pred)\n",
    "\n",
    "print(\"===== Улучшенный бейзлайн (Логист. регрессия) =====\")\n",
    "print(\"Лучшие параметры:\", grid_logreg.best_params_)\n",
    "print(f\"Accuracy:  {acc_best:.4f}\")\n",
    "print(f\"Precision: {prec_best:.4f}\")\n",
    "print(f\"Recall:    {rec_best:.4f}\")\n",
    "print(f\"F1-score:  {f1_best:.4f}\")\n",
    "\n",
    "# ----- (3) УЛУЧШЕНИЕ ДЛЯ РЕГРЕССИИ: RIDGE + ПОЛИНОМИАЛЬНЫЕ ПРИЗНАКИ -----\n",
    "\n",
    "# Целевая переменная: G3\n",
    "X_reg = data[['G1', 'G2', 'studytime', 'failures', 'absences', 'total_G1_G2']]\n",
    "y_reg = data['G3']\n",
    "\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Пайплайн: масштабируем, делаем полиномиальные признаки, потом Ridge\n",
    "pipe_ridge = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(include_bias=False)),\n",
    "    ('ridge', Ridge(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_ridge = {\n",
    "    'scaler': [StandardScaler(), MinMaxScaler(), RobustScaler()],\n",
    "    'poly__degree': [1, 2],  # пробуем линейные и квадратичные признаки\n",
    "    'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid_ridge = GridSearchCV(\n",
    "    pipe_ridge, param_grid_ridge,\n",
    "    cv=5, scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_ridge.fit(Xr_train, yr_train)\n",
    "best_ridge = grid_ridge.best_estimator_\n",
    "yr_pred = best_ridge.predict(Xr_test)\n",
    "\n",
    "mse_best = mean_squared_error(yr_test, yr_pred)\n",
    "mae_best = mean_absolute_error(yr_test, yr_pred)\n",
    "r2_best = r2_score(yr_test, yr_pred)\n",
    "\n",
    "print(\"\\n===== Улучшенный бейзлайн (Ridge регрессия) =====\")\n",
    "print(\"Лучшие параметры:\", grid_ridge.best_params_)\n",
    "print(f\"MSE:  {mse_best:.4f}\")\n",
    "print(f\"MAE:  {mae_best:.4f}\")\n",
    "print(f\"R^2:  {r2_best:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**:\n",
    "- Мы проверили гипотезу о том, что при логистической регрессии поможет масштабирование и подбор `C`, `penalty`, `solver`.  \n",
    "- Для линейной регрессии (заменили на Ridge) попробовали масштабирование, полиномиальные признаки и гиперпараметр `alpha`.  \n",
    "- Сравните полученные метрики с бейзлайном из пункта 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Имплементация алгоритма машинного обучения\n",
    "**4.a** Самостоятельно имплементировать алгоритмы (логистическую и линейную регрессию)  \n",
    "**4.b** Обучить имплементированные модели (логистическая и линейная)  \n",
    "**4.c** Оценить качество имплементированных моделей  \n",
    "**4.d** Сравнить результаты имплементированных моделей с результатами из пункта 2  \n",
    "**4.e** Сделать выводы  \n",
    "**4.f** Добавить техники из улучшенного бейзлайна (пункт 3.c)  \n",
    "**4.g** Обучить модели (логистическую и линейную)  \n",
    "**4.h** Оценить качество этих моделей  \n",
    "**4.i** Сравнить результаты моделей с пунктом 3  \n",
    "**4.j** Сделать выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Custom Logistic Regression - Classification (без масштабирования) ===\n",
      "Accuracy:  0.8228\n",
      "Precision: 0.8421\n",
      "Recall:    0.9057\n",
      "F1-score:  0.8727\n",
      "\n",
      "=== Custom Linear Regression - Regression (без масштабирования) ===\n",
      "MSE:  4.2771\n",
      "MAE:  1.2219\n",
      "R^2:  0.7914\n",
      "\n",
      "===Масштабирование===\n",
      "\n",
      "=== Custom Logistic Regression - Classification (scaled) ===\n",
      "Accuracy:  0.8354\n",
      "F1-score:  0.8713\n",
      "\n",
      "=== Custom Linear Regression - Regression (scaled) ===\n",
      "MSE:  4.4633\n",
      "R^2:  0.7823\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "####################\n",
    "# 4.1 Имплементация ЛИНЕЙНОЙ РЕГРЕССИИ\n",
    "####################\n",
    "class CustomLinearRegression:\n",
    "    def __init__(self, lr=0.01, n_iter=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X, dtype=float)\n",
    "        y = np.array(y, dtype=float)\n",
    "        \n",
    "        # Добавляем столбец 1 (bias)\n",
    "        ones = np.ones((X.shape[0], 1))\n",
    "        X = np.hstack([ones, X])\n",
    "        \n",
    "        # Инициализация весов\n",
    "        self.w_ = np.zeros(X.shape[1])\n",
    "        \n",
    "        # Градиентный спуск\n",
    "        for _ in range(self.n_iter):\n",
    "            y_pred = X.dot(self.w_)\n",
    "            error = y_pred - y\n",
    "            grad = (1 / X.shape[0]) * X.T.dot(error)\n",
    "            self.w_ -= self.lr * grad\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.array(X, dtype=float)\n",
    "        ones = np.ones((X.shape[0], 1))\n",
    "        X = np.hstack([ones, X])\n",
    "        return X.dot(self.w_)\n",
    "\n",
    "####################\n",
    "# 4.2 Имплементация ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ\n",
    "####################\n",
    "class CustomLogisticRegression:\n",
    "    def __init__(self, lr=0.0001, n_iter=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X, dtype=float)\n",
    "        y = np.array(y, dtype=float)\n",
    "        \n",
    "        # Добавляем столбец 1 (bias)\n",
    "        ones = np.ones((X.shape[0], 1))\n",
    "        X = np.hstack([ones, X])\n",
    "        \n",
    "        self.w_ = np.zeros(X.shape[1])\n",
    "        \n",
    "        for _ in range(self.n_iter):\n",
    "            z = X.dot(self.w_)\n",
    "            y_pred = self._sigmoid(z)\n",
    "            error = y_pred - y\n",
    "            grad = (1 / X.shape[0]) * X.T.dot(error)\n",
    "            self.w_ -= self.lr * grad\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X, dtype=float)\n",
    "        ones = np.ones((X.shape[0], 1))\n",
    "        X = np.hstack([ones, X])\n",
    "        return self._sigmoid(X.dot(self.w_))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba >= threshold).astype(int)\n",
    "\n",
    "# ------------------------\n",
    "# ШАГ 4. ОБУЧАЕМ КАСТОМНЫЕ МОДЕЛИ (без масштабирования)\n",
    "# ------------------------\n",
    "\n",
    "# 4.a Логистическая регрессия (Classification)\n",
    "custom_logreg = CustomLogisticRegression(lr=0.01, n_iter=2000)\n",
    "custom_logreg.fit(Xc_train, yc_train)\n",
    "yc_pred_custom = custom_logreg.predict(Xc_test)\n",
    "\n",
    "acc_custom = accuracy_score(yc_test, yc_pred_custom)\n",
    "prec_custom = precision_score(yc_test, yc_pred_custom)\n",
    "rec_custom = recall_score(yc_test, yc_pred_custom)\n",
    "f1_custom = f1_score(yc_test, yc_pred_custom)\n",
    "\n",
    "print(\"\\n=== Custom Logistic Regression - Classification (без масштабирования) ===\")\n",
    "print(f\"Accuracy:  {acc_custom:.4f}\")\n",
    "print(f\"Precision: {prec_custom:.4f}\")\n",
    "print(f\"Recall:    {rec_custom:.4f}\")\n",
    "print(f\"F1-score:  {f1_custom:.4f}\")\n",
    "\n",
    "# 4.b Линейная регрессия (Regression)\n",
    "custom_linreg = CustomLinearRegression(lr=0.001, n_iter=2000)\n",
    "custom_linreg.fit(Xr_train, yr_train)\n",
    "yr_pred_custom = custom_linreg.predict(Xr_test)\n",
    "\n",
    "mse_custom = mean_squared_error(yr_test, yr_pred_custom)\n",
    "mae_custom = mean_absolute_error(yr_test, yr_pred_custom)\n",
    "r2_custom = r2_score(yr_test, yr_pred_custom)\n",
    "\n",
    "print(\"\\n=== Custom Linear Regression - Regression (без масштабирования) ===\")\n",
    "print(f\"MSE:  {mse_custom:.4f}\")\n",
    "print(f\"MAE:  {mae_custom:.4f}\")\n",
    "print(f\"R^2:  {r2_custom:.4f}\")\n",
    "\n",
    "# ------------------------\n",
    "# ШАГ 5. УЛУЧШЕНИЕ БЕЙЗЛАЙНА (МАСШТАБИРОВАНИЕ)\n",
    "# ------------------------\n",
    "scaler_class = StandardScaler()\n",
    "Xc_train_scaled = scaler_class.fit_transform(Xc_train)\n",
    "Xc_test_scaled = scaler_class.transform(Xc_test)\n",
    "\n",
    "scaler_reg = StandardScaler()\n",
    "Xr_train_scaled = scaler_reg.fit_transform(Xr_train)\n",
    "Xr_test_scaled = scaler_reg.transform(Xr_test)\n",
    "\n",
    "# 5.a Логистическая регрессия (scaled)\n",
    "custom_logreg_scaled = CustomLogisticRegression(lr=0.01, n_iter=2000)\n",
    "custom_logreg_scaled.fit(Xc_train_scaled, yc_train)\n",
    "yc_pred_custom_scaled = custom_logreg_scaled.predict(Xc_test_scaled)\n",
    "\n",
    "acc_custom_s = accuracy_score(yc_test, yc_pred_custom_scaled)\n",
    "f1_custom_s = f1_score(yc_test, yc_pred_custom_scaled)\n",
    "\n",
    "print(\"\\n===Масштабирование===\")\n",
    "print(\"\\n=== Custom Logistic Regression - Classification (scaled) ===\")\n",
    "print(f\"Accuracy:  {acc_custom_s:.4f}\")\n",
    "print(f\"F1-score:  {f1_custom_s:.4f}\")\n",
    "\n",
    "# 5.b Линейная регрессия (scaled)\n",
    "custom_linreg_scaled = CustomLinearRegression(lr=0.01, n_iter=2000)\n",
    "custom_linreg_scaled.fit(Xr_train_scaled, yr_train)\n",
    "yr_pred_custom_scaled = custom_linreg_scaled.predict(Xr_test_scaled)\n",
    "\n",
    "mse_custom_s = mean_squared_error(yr_test, yr_pred_custom_scaled)\n",
    "r2_custom_s = r2_score(yr_test, yr_pred_custom_scaled)\n",
    "\n",
    "print(\"\\n=== Custom Linear Regression - Regression (scaled) ===\")\n",
    "print(f\"MSE:  {mse_custom_s:.4f}\")\n",
    "print(f\"R^2:  {r2_custom_s:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
